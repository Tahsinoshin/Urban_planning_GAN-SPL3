{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2ZJEPT57Zo8tLFqnMkebI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhpQ9Yvvw4iz","executionInfo":{"status":"ok","timestamp":1694293577664,"user_tz":-360,"elapsed":3836,"user":{"displayName":"Samiha Tahsin Noshin","userId":"05800708276661648633"}},"outputId":"bb207f3a-55f2-45f2-fa6a-34d9d84d020a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Step [10/100], x = 0.0762, f(x) = 0.0236\n","Step [20/100], x = -0.2712, f(x) = 0.0746\n","Step [30/100], x = -0.0340, f(x) = 0.0039\n","Step [40/100], x = 0.1008, f(x) = 0.0105\n","Step [50/100], x = -0.0048, f(x) = 0.0000\n","Step [60/100], x = -0.0347, f(x) = 0.0015\n","Step [70/100], x = 0.0147, f(x) = 0.0001\n","Step [80/100], x = 0.0054, f(x) = 0.0001\n","Step [90/100], x = -0.0080, f(x) = 0.0001\n","Step [100/100], x = 0.0029, f(x) = 0.0000\n","Optimized x = 0.0029, f(x) = 0.0000\n"]}],"source":["import torch\n","import torch.optim as optim\n","\n","# Define a simple quadratic function to minimize: f(x) = x^2\n","def quadratic_function(x):\n","    return x**2\n","\n","# Initialize the parameter 'x' that we want to optimize\n","x = torch.tensor([1.0], requires_grad=True)\n","\n","# Define the Adam optimizer\n","learning_rate = 0.1\n","optimizer = optim.Adam([x], lr=learning_rate)\n","\n","# Number of optimization steps\n","num_steps = 100\n","\n","# Optimization loop\n","for step in range(num_steps):\n","    # Forward pass: Compute the function value\n","    y = quadratic_function(x)\n","\n","    # Backward pass: Compute gradients\n","    y.backward()\n","\n","    # Update the parameter 'x' using the Adam optimizer\n","    optimizer.step()\n","\n","    # Zero the gradients\n","    optimizer.zero_grad()\n","\n","    # Print the progress\n","    if (step + 1) % 10 == 0:\n","        print(f\"Step [{step + 1}/{num_steps}], x = {x.item():.4f}, f(x) = {y.item():.4f}\")\n","\n","# Print the final result\n","print(f\"Optimized x = {x.item():.4f}, f(x) = {quadratic_function(x).item():.4f}\")\n"]}]}